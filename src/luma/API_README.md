# Luma Entity Extraction REST API

A Flask-based REST API for entity extraction with support for NER, grouping, intent mapping, and optional LLM fallback.

## üöÄ Quick Start

### Local Development

```bash
cd src
python luma/api.py
```

**API will be available at**: `http://localhost:9001`

### Docker

```bash
cd src/luma
docker compose up --build
```

### Production (Gunicorn)

```bash
cd src
gunicorn -w 4 -b 0.0.0.0:9001 luma.api:app
```

---

## üì° API Endpoints

### POST `/extract`

Extract entities from input text.

**Request:**
```json
{
    "text": "add 2 kg rice",
    "force_llm": false,        // optional: force LLM extraction
    "enable_fuzzy": false      // optional: enable fuzzy matching
}
```

**Response:**
```json
{
    "success": true,
    "data": {
        "status": "success",
        "original_sentence": "add 2 kg rice",
        "parameterized_sentence": "add 2 kg producttoken",
        "groups": [{
            "action": "add",
            "intent": "add",
            "intent_confidence": 0.98,
            "products": ["rice"],
            "quantities": ["2"],
            "units": ["kg"],
            "brands": [],
            "variants": [],
            "ordinal_ref": null
        }],
        "grouping_result": {
            "status": "ok",
            "reason": null,
            "route": "rule"
        },
        "notes": "Luma pipeline (route=rule)"
    }
}
```

### GET `/health`

Check API health status.

**Response:**
```json
{
    "status": "healthy",
    "features": {
        "llm_fallback": false,
        "fuzzy_matching": false,
        "intent_mapping": true
    }
}
```

### GET `/info`

Get API information and configuration.

**Response:**
```json
{
    "name": "Luma Entity Extraction API",
    "version": "1.0.0",
    "endpoints": {...},
    "features": {...},
    "configuration": {...}
}
```

---

## ‚öôÔ∏è Configuration

### Environment Variables

```bash
# Server
export PORT=9001

# Features
export ENABLE_LLM_FALLBACK=false
export ENABLE_FUZZY_MATCHING=false
export ENABLE_INTENT_MAPPER=true

# LLM Settings (if LLM enabled)
export OPENAI_API_KEY=your_key_here
export LLM_MODEL=gpt-4

# Debugging
export DEBUG_NLP=0
```

### Docker Compose

Edit `docker-compose.yml`:

```yaml
environment:
  ENABLE_LLM_FALLBACK: "true"
  OPENAI_API_KEY: "your-key-here"
```

---

## üß™ Testing

### Test Scripts

**Linux/Mac/WSL:**
```bash
cd src/luma
chmod +x test_api.sh
./test_api.sh
```

**Windows (PowerShell):**
```powershell
cd src\luma
.\test_api.ps1
```

### Manual Testing

**Simple extraction:**
```bash
curl -X POST http://localhost:9001/extract \
  -H "Content-Type: application/json" \
  -d '{"text": "add 2 kg rice"}'
```

**With force_llm:**
```bash
curl -X POST http://localhost:9001/extract \
  -H "Content-Type: application/json" \
  -d '{"text": "add rice", "force_llm": true}'
```

**Health check:**
```bash
curl http://localhost:9001/health
```

---

## üìä Response Fields

### Success Response

```json
{
    "success": true,
    "data": {
        "status": "success|error|needs_llm",
        "original_sentence": "...",
        "parameterized_sentence": "...",
        "groups": [...],
        "grouping_result": {
            "status": "ok|needs_llm_fix",
            "reason": null,
            "route": "rule|memory|llm"
        },
        "notes": "..."
    }
}
```

### Error Response

```json
{
    "success": false,
    "error": "Error description"
}
```

### Group Structure

```json
{
    "action": "add|remove|set|check",
    "intent": "add|remove|...",
    "intent_confidence": 0.98,
    "products": ["rice"],
    "quantities": ["2"],
    "units": ["kg"],
    "brands": [],
    "variants": [],
    "ordinal_ref": null
}
```

---

## üéØ Processing Routes

The API returns a `route` field indicating how the request should be handled:

| Route | Description | Example |
|-------|-------------|---------|
| **rule** | Rule-based extraction successful | "add 2 kg rice" |
| **memory** | Needs conversation memory | "add it", "remove that" |
| **llm** | Needs LLM for complex cases | "I want some food" |

**Usage:**
```python
result = pipeline.extract("add it")
if result.grouping_result.route == "memory":
    # Resolve from conversation state
    product = resolve_from_memory(state)
elif result.grouping_result.route == "llm":
    # Use LLM extraction
    result = pipeline.extract(text, force_llm=True)
```

---

## üîß Integration Examples

### Python Client

```python
import requests

def extract_entities(text):
    response = requests.post(
        "http://localhost:9001/extract",
        json={"text": text}
    )
    return response.json()

# Use it
result = extract_entities("add 2 kg rice")
if result["success"]:
    groups = result["data"]["groups"]
    print(groups[0]["products"])  # ['rice']
```

### JavaScript/Node.js Client

```javascript
async function extractEntities(text) {
    const response = await fetch('http://localhost:9001/extract', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ text })
    });
    return await response.json();
}

// Use it
const result = await extractEntities('add 2 kg rice');
if (result.success) {
    console.log(result.data.groups[0].products); // ['rice']
}
```

### cURL Client

```bash
curl -X POST http://localhost:9001/extract \
  -H "Content-Type: application/json" \
  -d '{"text": "add 2 kg rice"}' \
  | jq '.data.groups[0].products'
```

---

## üê≥ Docker Deployment

### Build and Run

```bash
cd src/luma
docker compose up --build
```

### Stop

```bash
docker compose down
```

### View Logs

```bash
docker compose logs -f
```

### Custom Port

```bash
PORT=8080 docker compose up
```

---

## üìà Performance

### Startup Time
- **Cold start**: ~5-10 seconds (model loading)
- **Subsequent requests**: <100ms

### Throughput
- **Simple extractions**: ~50-100 requests/second
- **With LLM fallback**: ~1-5 requests/second (depends on LLM)

### Memory Usage
- **Base**: ~500MB (models loaded)
- **Peak**: ~800MB (processing)

---

## üîí Security Notes

### Production Recommendations

1. **Authentication**: Add API key middleware
2. **Rate Limiting**: Use Flask-Limiter
3. **Input Validation**: Already implemented
4. **HTTPS**: Use reverse proxy (nginx, Caddy)
5. **CORS**: Configure if needed

### Example: Add API Key Auth

```python
from functools import wraps
from flask import request

API_KEY = os.getenv("API_KEY")

def require_api_key(f):
    @wraps(f)
    def decorated(*args, **kwargs):
        key = request.headers.get("X-API-Key")
        if key != API_KEY:
            return jsonify({"error": "Invalid API key"}), 401
        return f(*args, **kwargs)
    return decorated

@app.route("/extract", methods=["POST"])
@require_api_key
def extract():
    ...
```

---

## üêõ Troubleshooting

### Issue: API won't start

**Check:**
1. Port 9001 is available
2. Dependencies installed
3. spaCy model downloaded

```bash
python -c "import luma; print('Luma available')"
python -m spacy info en_core_web_sm
```

### Issue: Slow startup

**Cause**: Model loading takes time

**Solution**: Use Docker with pre-warmed models

### Issue: LLM fallback not working

**Check:**
1. `ENABLE_LLM_FALLBACK=true`
2. `OPENAI_API_KEY` is set
3. Internet connectivity

---

## üìù Examples

### Example 1: E-commerce Cart

```bash
curl -X POST http://localhost:9001/extract \
  -H "Content-Type: application/json" \
  -d '{
    "text": "Add 2 bags of Mama Gold rice, 3 tins of Peak milk, and a crate of Coca-Cola to my cart"
  }' | jq '.data.groups'
```

**Response:**
```json
[
  {
    "action": "add",
    "products": ["mama gold rice"],
    "quantities": ["2"],
    "units": ["bags"],
    "brands": ["mama gold"]
  },
  {
    "action": "add",
    "products": ["peak milk"],
    "quantities": ["3"],
    "units": ["tins"],
    "brands": ["peak"]
  },
  {
    "action": "add",
    "products": ["coca-cola"],
    "quantities": ["1"],
    "units": ["crate"],
    "brands": ["coca-cola"]
  }
]
```

### Example 2: Availability Check

```bash
curl -X POST http://localhost:9001/extract \
  -H "Content-Type: application/json" \
  -d '{"text": "do you have Dangote sugar in stock?"}' \
  | jq '.data.groups[0].intent'
```

**Response:**
```json
"check"
```

---

## üéâ Summary

The Luma API provides:
- ‚úÖ Fast, accurate entity extraction
- ‚úÖ Intent classification
- ‚úÖ Ordinal reference detection
- ‚úÖ Optional LLM fallback
- ‚úÖ Optional fuzzy matching
- ‚úÖ Docker-ready deployment
- ‚úÖ Comprehensive error handling

**Ready for production use!** üöÄ

