services:
  luma-api:
    build:
      context: ..
      dockerfile: luma/Dockerfile
    ports:
      - "9001:9001"
    environment:
      # Core settings
      PORT: "9001"
      DEBUG_NLP: "0"
      
      # Feature toggles
      ENABLE_LLM_FALLBACK: "false"
      ENABLE_FUZZY_MATCHING: "false"
      ENABLE_INTENT_MAPPER: "true"
      
      # Logging settings
      LOG_LEVEL: "INFO"
      LOG_FORMAT: "json"  # Use 'json' for production, 'pretty' for dev
      ENABLE_REQUEST_LOGGING: "true"
      LOG_PERFORMANCE_METRICS: "true"
      # LOG_FILE: "/var/log/luma/api.log"  # Optional: enable for file-based logs
      
      # LLM settings (if enabled)
      # OPENAI_API_KEY: "your-key-here"
      LLM_MODEL: "gpt-4"
    
    # Optional: Mount volume for file-based logs
    # volumes:
    #   - ./logs:/var/log/luma
    
    # Docker logging configuration
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        tag: "luma-api"
    
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9001/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
